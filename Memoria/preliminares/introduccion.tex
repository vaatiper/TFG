% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción
%*******************************************************

% \manualmark
% \markboth{\textsc{Introducción}}{\textsc{Introducción}} 

\chapter{Introducción}

En este trabajo se estudian los aspectos más importantes de la Teoría de Distribuciones con el objetivo de comprender, desarrollar e implementar los resultados propuestos en el artículo~\cite{chen1995universal} para la aproximación mediante redes neuronales de operadores de Lipschitz continuos entre espacios de Banach. 


El trabajo comienza por el estudio de los Espacios Vectoriales Topológicos (EVT), nombre que se le da a los espacios vectoriales dotados de una topología vectorial. En el \autoref{ch:tercer-capitulo}, después de hacer una presentación constructiva de ellos, se demuestra el Teorema de Riesz para EVT separados. La importancia de este resultado reside en que demuestra la falta de compactos en espacios de dimensión infinita, lo que justificará la introducción de las topologías débil y débil-* más adelante. Tras esta introducción, se exponen las nociones principales de convergencia en EVT y se presentan las características de los tipos de EVT de los que se hará uso durante el trabajo. A continuación, se introduce el dual de un EVT y se proponen las topologías débil y débil-* para preservar la continuidad de funcionales del dual y garantizar la presencia de más conjuntos compactos que con la topología de la norma. El capítulo finaliza con la introducción del espacio de funciones test. 

El \autoref{ch:cuarto-capitulo} comprende los contenidos trabajados sobre la Teoría de Distribuciones. En él se introduce el concepto de distribución como función generalizada y su compatibilidad con el cálculo diferencial clásico. Posteriormente, se llega a una definición de transformada de Fourier compatible con las distribuciones, para lo que se estudian las funciones de decrecimiento rápido y las distribuciones temperadas. 

Para la realización de los capítulos mencionados hasta ahora, las principales fuentes de información que se han consultado son: 

\begin{itemize}
    \item \emph{Functional Analysis}, por Walter Rudin~\cite{rudin1991functional}.
    \item \emph{Análisis Funcional}, por Miguel Martín Suárez~\cite{martin2011analisis}.
    \item Material proporcionado para el curso Análisis Funcional Avanzado impartido en la titulación de máster FisyMat de la Universidad de Granada, curso 2023/2024.
    \item \emph{Topological Vector Spaces I}, por  G. Köthe~\cite{kothe1979topological}.
    \item \emph{Fourier Transform of Meassures}, por Toru Maruyama~\cite{Maruyama2018}.

     
    
\end{itemize}



En el \autoref{ch:quinto-capitulo} se realiza un estudio de los fundamentos del aprendizaje profundo. Para ello, se describe un modelo formal de  aprendizaje automático y se profundiza en el modelo de red neuronal prealimentada y en su arquitectura, aplicaciones y técnicas de optimización. Para la realización de este capítulo, las principales fuentes de consulta son: 

\begin{itemize}
    \item \emph{Deep Learning}, por Goodfellow, Bengio y Courville~\cite{goodfellow2016deep}.
    \item \emph{Understanding Machine Learning: From Theory to Algorithms}, por Shai Shalev-Shwartz y Shai Ben-David~\cite{shalev-shwartz2014understanding}.
    \item Material proporcionado para el curso de Aprendizaje Automático del Grado en Ingeniería Informática de la Universidad de Granada, curso 2023/2024. 

    
\end{itemize}

  
Llegados a este punto, se han adquirido los conocimientos teóricos necesarios para abordar los resultados que se proponen en~\cite{chen1995universal}. En el \autoref{ch:sexto-capitulo} se desarrolla la demostración de todos los resultados que aparecen en este artículo. Los primeros resultados tienen como objetivo debilitar las condiciones bajo las cuales una función puede ser considerada función de activación, mientras que los resultados posteriores aprovechan esta debilitación para establecer teoremas de aproximación tanto para funcionales como para operadores. Al final del capítulo se propone un ejemplo de la arquitectura dada por el teorema aplicado a sistemas dinámicos. 

Los resultados del \autoref{ch:sexto-capitulo} no son constructivos, pero la arquitectura general propuesta para la aproximación de sistemas dinámicos recuerda al enfoque de las Physical-Informed Neural Networks (PINNs). Es por esto que otro objetivo del trabajo es mostrar cómo funcionan las PINNs a través \textit{SciANN}~\cite{Haghighat2021}, una de las librerías más utilizadas en el ámbito de las PINNs, y estudiar la viabilidad de aproximar operadores a través de ella. En el \autoref{ch:septimo-capitulo} se realiza un estudio sobre las características de las PINNs y se relaciona con las particularidades de la librería \textit{SciANN}. 

Por último, en el \autoref{ch:octavo-capitulo} se realizan varias experimentaciones que muestran tanto la versatilidad de \textit{SciANN} para abordar distintos tipos de problemas como sus limitaciones, concluyendo que aún no dispone de las abstracciones necesarias para soportar el aprendizaje de operadores.







\endinput
